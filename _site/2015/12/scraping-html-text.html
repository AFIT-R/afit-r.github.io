<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width initial-scale=1" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

    <title>Scraping HTML Text</title>
    <meta name="description" content="">

    <link rel="stylesheet" href="http://bradleyboehmke.github.io//css/main.css">
    <link rel="canonical" href="http://bradleyboehmke.github.io/http://bradleyboehmke.github.io//2015/12/scraping-html-text.html">
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="http://bradleyboehmke.github.io/">Bradley Boehmke</a>

    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg viewBox="0 0 18 15">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>

      <div class="trigger">
        
          
          <a class="page-link" href="http://bradleyboehmke.github.io//about/">About</a>
          
        
          
        
          
        
          
        
          
          <a class="page-link" href="http://bradleyboehmke.github.io//categoryview/">Archive</a>
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
          <a class="page-link" href="http://bradleyboehmke.github.io//cv/">CV</a>
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
          <a class="page-link" href="http://bradleyboehmke.github.io//tutorials/">Tutorials</a>
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
          <a class="page-link" href="http://bradleyboehmke.github.io//tutorials/scraping_with_apis_DSCOE.Rmd">Scraping Data with APIs</a>
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      </div>
    </nav>

  </div>

</header>


    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">Scraping HTML Text</h1>
    <p class="post-meta">Dec 14, 2015 &nbsp; • &nbsp; Categories: 
      
            &nbsp;<a href="http://bradleyboehmke.github.io/categoryview/#programming" style="color: rgb(128,128,128)">programming</a>
      
       &nbsp; • &nbsp; Tags: 
      
            &nbsp;<a href="http://bradleyboehmke.github.io/categoryview/#r" style="color: rgb(128,128,128)">r</a>
      
            &nbsp;<a href="http://bradleyboehmke.github.io/categoryview/#rvest" style="color: rgb(128,128,128)">rvest</a>
      
            &nbsp;<a href="http://bradleyboehmke.github.io/categoryview/#web-scraping" style="color: rgb(128,128,128)">web scraping</a>
      
    </p>
      
      <br>
      
  </header>

  <article class="post-content">
    <style type="text/css"> 
<!-- 
        .indented { 
                padding-left: 25pt; 
                padding-right: 50pt; 
                } 
--> 
</style>

<p><a href="http://bradleyboehmke.github.io/2015/12/scraping-html-text.html"><img src="http://d1u2s20mo6at4b.cloudfront.net/wp-content/uploads/HTML.jpg" alt="Scraping HTML Text" style="float:left; margin:2px 8px 0px 0px; width: 17%; height: 17%;" /></a>
Vast amount of information exists across the interminable webpages that exist online.  Much of this information are considered “unstructured” texts since they don’t come in a neatly packaged speadsheet. Fortunately, HTML websites are organized documents which means these texts are actually structured within underlying HTML code elements…we just need to figure out how to extract it! This post covers the basics of scraping text from online sources.<!--more--></p>

<p><br /></p>

<h2 id="tldr">tl;dr</h2>
<p>Short on time? Here’s the gist. Throughout this post I illustrate how to use the <code class="highlighter-rouge">rvest</code> package to extract different text components of webpages by dissecting the <a href="https://en.wikipedia.org/wiki/Web_scraping">Wikipedia page on web scraping</a>. As I foray I cover:</p>

<p class="indented">
        <a href="#html_nodes">&#9312;</a> Basic knowledge of HTML element components you'll need to scrape with rvest
        <br />
        <a href="#scraping_nodes">&#9313;</a> How to extract text from the common HTML nodes in a webpage
        <br />
        <a href="#specific_nodes">&#9314;</a> How to extract text from specific HTML nodes of interest
        <br />
        <a href="#cleaning">&#9315;</a> Some of the common text cleaning that is required post-scraping
</p>

<p><br /></p>

<p><a name="html_nodes"></a></p>

<h2 id="html-nodes">① HTML Nodes</h2>
<p>However, its important to first cover one of the basic components of HTML elements as we will leverage this information to pull desired information. I offer only enough insight required to begin scraping; I highly recommend <a href="http://www.amazon.com/XML-Web-Technologies-Data-Sciences/dp/1461478995"><em>XML and Web Technologies for Data Sciences with R</em></a> and <a href="http://www.amazon.com/Automated-Data-Collection-Practical-Scraping/dp/111883481X/ref=pd_sim_14_1?ie=UTF8&amp;dpID=51Tm7FHxWBL&amp;dpSrc=sims&amp;preST=_AC_UL160_SR108%2C160_&amp;refRID=1VJ1GQEY0VCPZW7VKANX"><em>Automated Data Collection with R</em></a> to learn more about HTML and XML element structures.</p>

<p>HTML elements are written with a start tag, an end tag, and with the content in between: <code class="highlighter-rouge">&lt;tagname&gt;content&lt;/tagname&gt;</code>. The tags which typically contain the textual content we wish to scrape, and the tags we will leverage in the next two sections, include:</p>

<ul>
  <li><code class="highlighter-rouge">&lt;h1&gt;</code>, <code class="highlighter-rouge">&lt;h2&gt;</code>,…,<code class="highlighter-rouge">&lt;h6&gt;</code>: Largest heading, second largest heading, etc.</li>
  <li><code class="highlighter-rouge">&lt;p&gt;</code>: Paragraph elements</li>
  <li><code class="highlighter-rouge">&lt;ul&gt;</code>: Unordered bulleted list</li>
  <li><code class="highlighter-rouge">&lt;ol&gt;</code>: Ordered list</li>
  <li><code class="highlighter-rouge">&lt;li&gt;</code>: Individual List item</li>
  <li><code class="highlighter-rouge">&lt;div&gt;</code>: Division or section</li>
  <li><code class="highlighter-rouge">&lt;table&gt;</code>: Table</li>
</ul>

<p>For example, text in paragraph form that you see online is wrapped with the HTML paragraph tag <code class="highlighter-rouge">&lt;p&gt;</code> as in:</p>

<figure class="highlight"><pre><code class="language-r" data-lang="r"><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span>
<span class="n">This</span> <span class="n">paragraph</span> <span class="n">represents</span>
<span class="n">a</span> <span class="n">typical</span> <span class="n">text</span> <span class="n">paragraph</span>
<span class="k">in</span> <span class="n">HTML</span> <span class="n">form</span>
<span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span></code></pre></figure>

<p>It is through these tags that we can start to extract textual components (also referred to as nodes) of HTML webpages.</p>

<p><br /></p>

<p><a name="scraping_nodes"></a></p>

<h2 id="scraping-html-nodes">② Scraping HTML Nodes</h2>
<p>To scrape online text we’ll make use of the relatively newer <a href="https://cran.r-project.org/web/packages/rvest/index.html"><code class="highlighter-rouge">rvest</code></a> package. <code class="highlighter-rouge">rvest</code> was created by the RStudio team inspired by libraries such as <a href="http://www.crummy.com/software/BeautifulSoup/">beautiful soup</a> which has greatly simplified web scraping. <code class="highlighter-rouge">rvest</code> provides multiple functionalities; however, in this section we will focus only on extracting HTML text with <code class="highlighter-rouge">rvest</code>. Its important to note that <code class="highlighter-rouge">rvest</code> makes use of of the pipe operator (<code class="highlighter-rouge">%&gt;%</code>) developed through the <a href="https://cran.r-project.org/web/packages/magrittr/index.html"><code class="highlighter-rouge">magrittr</code> package</a>. If you are not familiar with the functionality of <code class="highlighter-rouge">%&gt;%</code> I recommend you jump to the tutorial on <a href="#pipe">Simplifying Your Code with <code class="highlighter-rouge">%&gt;%</code></a> so that you have a better understanding of what’s going on with the code.</p>

<p>To extract text from a webpage of interest, we specify what HTML elements we want to select by using <code class="highlighter-rouge">html_nodes()</code>.  For instance, if we want to scrape the primary heading for the <a href="https://en.wikipedia.org/wiki/Web_scraping">Web Scraping Wikipedia webpage</a> we simply identify the <code class="highlighter-rouge">&lt;h1&gt;</code> node as the node we want to select.  <code class="highlighter-rouge">html_nodes()</code> will identify all <code class="highlighter-rouge">&lt;h1&gt;</code> nodes on the webpage and return the HTML element.  In our example we see there is only one <code class="highlighter-rouge">&lt;h1&gt;</code> node on this webpage.</p>

<figure class="highlight"><pre><code class="language-r" data-lang="r"><span class="n">library</span><span class="p">(</span><span class="n">rvest</span><span class="p">)</span>

<span class="n">scraping_wiki</span> <span class="o">&lt;-</span> <span class="n">read_html</span><span class="p">(</span><span class="s2">"https://en.wikipedia.org/wiki/Web_scraping"</span><span class="p">)</span>

<span class="n">scraping_wiki</span> <span class="o">%&gt;%</span>
        <span class="n">html_nodes</span><span class="p">(</span><span class="s2">"h1"</span><span class="p">)</span>
<span class="c1">## {xml_nodeset (1)}
</span><span class="err">##</span> <span class="p">[</span><span class="m">1</span><span class="p">]</span> <span class="o">&lt;</span><span class="n">h1</span> <span class="n">id</span><span class="o">=</span><span class="s2">"firstHeading"</span> <span class="n">class</span><span class="o">=</span><span class="s2">"firstHeading"</span> <span class="n">lang</span><span class="o">=</span><span class="s2">"en"</span><span class="o">&gt;</span><span class="n">Web</span> <span class="n">scraping</span><span class="o">&lt;/</span><span class="n">h1</span><span class="o">&gt;</span></code></pre></figure>

<p>To extract only the heading text for this <code class="highlighter-rouge">&lt;h1&gt;</code> node, and not include all the HTML syntax we use <code class="highlighter-rouge">html_text()</code> which returns the heading text we see at the top of the <a href="https://en.wikipedia.org/wiki/Web_scraping">Web Scraping Wikipedia page</a>.</p>

<figure class="highlight"><pre><code class="language-r" data-lang="r"><span class="n">scraping_wiki</span> <span class="o">%&gt;%</span>
        <span class="n">html_nodes</span><span class="p">(</span><span class="s2">"h1"</span><span class="p">)</span> <span class="o">%&gt;%</span>
        <span class="n">html_text</span><span class="p">()</span>
<span class="err">##</span> <span class="p">[</span><span class="m">1</span><span class="p">]</span> <span class="s2">"Web scraping"</span></code></pre></figure>

<p>If we want to identify all the second level headings on the webpage we follow the same process but instead select the <code class="highlighter-rouge">&lt;h2&gt;</code> nodes.  In this example we see there are 10 second level headings on the <a href="https://en.wikipedia.org/wiki/Web_scraping">Web Scraping Wikipedia page</a>.</p>

<figure class="highlight"><pre><code class="language-r" data-lang="r"><span class="n">scraping_wiki</span> <span class="o">%&gt;%</span>
        <span class="n">html_nodes</span><span class="p">(</span><span class="s2">"h2"</span><span class="p">)</span> <span class="o">%&gt;%</span>
        <span class="n">html_text</span><span class="p">()</span>
<span class="c1">##  [1] "Contents"                             
##  [2] "Techniques[edit]"                     
##  [3] "Legal issues[edit]"                   
##  [4] "Notable tools[edit]"                  
##  [5] "See also[edit]"                       
##  [6] "Technical measures to stop bots[edit]"
##  [7] "Articles[edit]"                       
##  [8] "References[edit]"                     
##  [9] "See also[edit]"                       
</span><span class="err">##</span> <span class="p">[</span><span class="m">10</span><span class="p">]</span> <span class="s2">"Navigation menu"</span></code></pre></figure>

<p>Next, we can move on to extracting much of the text on this webpage which is in paragraph form.  We can follow the same process illustrated above but instead we’ll select all <code class="highlighter-rouge">&lt;p&gt;</code>  nodes.  This selects the 17 paragraph elements from the web page; which we can examine by subsetting the list <code class="highlighter-rouge">p_nodes</code> to see the first line of each paragraph along with the HTML syntax. Just as before, to extract the text from these nodes and coerce them to a character string we simply apply <code class="highlighter-rouge">html_text()</code>.</p>

<figure class="highlight"><pre><code class="language-r" data-lang="r"><span class="n">p_nodes</span> <span class="o">&lt;-</span> <span class="n">scraping_wiki</span> <span class="o">%&gt;%</span> 
        <span class="n">html_nodes</span><span class="p">(</span><span class="s2">"p"</span><span class="p">)</span>

<span class="n">length</span><span class="p">(</span><span class="n">p_nodes</span><span class="p">)</span>
<span class="c1">## [1] 17
</span>
<span class="n">p_nodes</span><span class="p">[</span><span class="m">1</span><span class="o">:</span><span class="m">6</span><span class="p">]</span>
<span class="c1">## {xml_nodeset (6)}
## [1] &lt;p&gt;&lt;b&gt;Web scraping&lt;/b&gt; (&lt;b&gt;web harvesting&lt;/b&gt; or &lt;b&gt;web data extract ...
## [2] &lt;p&gt;Web scraping is closely related to &lt;a href="/wiki/Web_indexing" t ...
## [3] &lt;p/&gt;
## [4] &lt;p/&gt;
## [5] &lt;p&gt;Web scraping is the process of automatically collecting informati ...
## [6] &lt;p&gt;Web scraping may be against the &lt;a href="/wiki/Terms_of_use" titl ...
</span>

<span class="n">p_text</span> <span class="o">&lt;-</span> <span class="n">scraping_wiki</span> <span class="o">%&gt;%</span>
        <span class="n">html_nodes</span><span class="p">(</span><span class="s2">"p"</span><span class="p">)</span> <span class="o">%&gt;%</span>
        <span class="n">html_text</span><span class="p">()</span>

<span class="n">p_text</span><span class="p">[</span><span class="m">1</span><span class="p">]</span>
<span class="err">##</span> <span class="p">[</span><span class="m">1</span><span class="p">]</span> <span class="s2">"Web scraping (web harvesting or web data extraction) is a computer software technique of extracting information from websites. Usually, such software programs simulate human exploration of the World Wide Web by either implementing low-level Hypertext Transfer Protocol (HTTP), or embedding a fully-fledged web browser, such as Mozilla Firefox."</span></code></pre></figure>

<p>Not too bad; however, we may not have captured all the text that we were hoping for.  Since we extracted text for all <code class="highlighter-rouge">&lt;p&gt;</code> nodes, we collected all identified paragraph text; however, this does not capture the text in the bulleted lists.  For example, when you look at the <a href="https://en.wikipedia.org/wiki/Web_scraping">Web Scraping Wikipedia page</a> you will notice a significant amount of text in bulleted list format following the third paragraph under the <strong><a href="https://en.wikipedia.org/wiki/Web_scraping#Techniques">Techniques</a></strong> heading.  If we look at our data we’ll see that that the text in this list format are not capture between the two paragraphs:</p>

<figure class="highlight"><pre><code class="language-text" data-lang="text">## Error in eval(expr, envir, enclos): object 'p_text' not found</code></pre></figure>

<figure class="highlight"><pre><code class="language-text" data-lang="text">## Error in eval(expr, envir, enclos): object 'p_text' not found</code></pre></figure>

<p>This is because the text in this list format are contained in <code class="highlighter-rouge">&lt;ul&gt;</code> nodes. To capture the text in lists, we can use the same steps as above but we select specific nodes which represent HTML lists components. We can approach extracting list text two ways.</p>

<p>First, we can pull all list elements (<code class="highlighter-rouge">&lt;ul&gt;</code>).  When scraping all <code class="highlighter-rouge">&lt;ul&gt;</code> text, the resulting data structure will be a character string vector with each element representing a single list consisting of all list items in that list.  In our running example there are 21 list elements as shown in the example that follows.  You can see the first list scraped is the table of contents and the second list scraped is the list in the <a href="https://en.wikipedia.org/wiki/Web_scraping#Techniques">Techniques</a> section.</p>

<figure class="highlight"><pre><code class="language-r" data-lang="r"><span class="n">ul_text</span> <span class="o">&lt;-</span> <span class="n">scraping_wiki</span> <span class="o">%&gt;%</span>
        <span class="n">html_nodes</span><span class="p">(</span><span class="s2">"ul"</span><span class="p">)</span> <span class="o">%&gt;%</span>
        <span class="n">html_text</span><span class="p">()</span>

<span class="n">length</span><span class="p">(</span><span class="n">ul_text</span><span class="p">)</span>
<span class="c1">## [1] 21
</span>
<span class="n">ul_text</span><span class="p">[</span><span class="m">1</span><span class="p">]</span>
<span class="c1">## [1] "\n1 Techniques\n2 Legal issues\n3 Notable tools\n4 See also\n5 Technical measures to stop bots\n6 Articles\n7 References\n8 See also\n"
</span>
<span class="c1"># read the first 200 characters of the second list
</span><span class="n">substr</span><span class="p">(</span><span class="n">ul_text</span><span class="p">[</span><span class="m">2</span><span class="p">],</span> <span class="n">start</span> <span class="o">=</span> <span class="m">1</span><span class="p">,</span> <span class="n">stop</span> <span class="o">=</span> <span class="m">200</span><span class="p">)</span>
<span class="err">##</span> <span class="p">[</span><span class="m">1</span><span class="p">]</span> <span class="s2">"\nHuman copy-and-paste: Sometimes even the best web-scraping technology cannot replace a human’s manual examination and copy-and-paste, and sometimes this may be the only workable solution when the web"</span></code></pre></figure>

<p>An alternative approach is to pull all <code class="highlighter-rouge">&lt;li&gt;</code> nodes.  This will pull the text contained in each list item for all the lists.  In our running example there’s 146 list items that we can extract from this Wikipedia page.  The first eight list items are the list of contents we see towards the top of the page. List items 9-17 are the list elements contained in the “<a href="https://en.wikipedia.org/wiki/Web_scraping#Techniques">Techniques</a>” section, list items 18-44 are the items listed under the “<a href="https://en.wikipedia.org/wiki/Web_scraping#Notable_tools">Notable Tools</a>” section, and so on.</p>

<figure class="highlight"><pre><code class="language-r" data-lang="r"><span class="n">li_text</span> <span class="o">&lt;-</span> <span class="n">scraping_wiki</span> <span class="o">%&gt;%</span>
        <span class="n">html_nodes</span><span class="p">(</span><span class="s2">"li"</span><span class="p">)</span> <span class="o">%&gt;%</span>
        <span class="n">html_text</span><span class="p">()</span>

<span class="n">length</span><span class="p">(</span><span class="n">li_text</span><span class="p">)</span>
<span class="c1">## [1] 147
</span>
<span class="n">li_text</span><span class="p">[</span><span class="m">1</span><span class="o">:</span><span class="m">8</span><span class="p">]</span>
<span class="c1">## [1] "1 Techniques"                      "2 Legal issues"                   
## [3] "3 Notable tools"                   "4 See also"                       
## [5] "5 Technical measures to stop bots" "6 Articles"                       
</span><span class="err">##</span> <span class="p">[</span><span class="m">7</span><span class="p">]</span> <span class="s2">"7 References"</span>                      <span class="s2">"8 See also"</span></code></pre></figure>

<p>At this point we may believe we have all the text desired and proceed with joining the paragraph (<code class="highlighter-rouge">p_text</code>) and list (<code class="highlighter-rouge">ul_text</code> or <code class="highlighter-rouge">li_text</code>) character strings and then perform the desired textual analysis.  However, we may now have captured <em>more</em> text than we were hoping for.  For example, by scraping all lists we are also capturing the listed links in the left margin of the webpage. If we look at the 104-136 list items that we scraped, we’ll see that these texts correspond to the left margin text.</p>

<figure class="highlight"><pre><code class="language-r" data-lang="r"><span class="n">li_text</span><span class="p">[</span><span class="m">104</span><span class="o">:</span><span class="m">136</span><span class="p">]</span>
<span class="c1">##  [1] "Main page"           "Contents"            "Featured content"   
##  [4] "Current events"      "Random article"      "Donate to Wikipedia"
##  [7] "Wikipedia store"     "Help"                "About Wikipedia"    
## [10] "Community portal"    "Recent changes"      "Contact page"       
## [13] "What links here"     "Related changes"     "Upload file"        
## [16] "Special pages"       "Permanent link"      "Page information"   
## [19] "Wikidata item"       "Cite this page"      "Create a book"      
## [22] "Download as PDF"     "Printable version"   "Català"             
## [25] "Deutsch"             "Español"             "Français"           
## [28] "Íslenska"            "Italiano"            "Latviešu"           
</span><span class="err">##</span> <span class="p">[</span><span class="m">31</span><span class="p">]</span> <span class="s2">"Nederlands"</span>          <span class="s2">"日本語"</span>              <span class="s2">"Српски / srpski"</span></code></pre></figure>

<p>If we desire to scrape every piece of text on the webpage than this won’t be of concern.  In fact, if we want to scrape all the text regardless of the content they represent there is an easier approach.  We can capture all the content to include text in paragraph (<code class="highlighter-rouge">&lt;p&gt;</code>), lists (<code class="highlighter-rouge">&lt;ul&gt;</code>, <code class="highlighter-rouge">&lt;ol&gt;</code>, and <code class="highlighter-rouge">&lt;li&gt;</code>), and even data in tables (<code class="highlighter-rouge">&lt;table&gt;</code>) by using <code class="highlighter-rouge">&lt;div&gt;</code>.  This is because these other elements are usually a subsidiary of an HTML division or section so pulling all <code class="highlighter-rouge">&lt;div&gt;</code> nodes will extract all text contained in that division or section regardless if it is also contained in a paragraph or list.</p>

<figure class="highlight"><pre><code class="language-r" data-lang="r"><span class="n">all_text</span> <span class="o">&lt;-</span> <span class="n">scraping_wiki</span> <span class="o">%&gt;%</span>
        <span class="n">html_nodes</span><span class="p">(</span><span class="s2">"div"</span><span class="p">)</span> <span class="o">%&gt;%</span> 
        <span class="n">html_text</span><span class="p">()</span></code></pre></figure>

<p><br /></p>

<p><a name="specific_nodes"></a></p>

<h2 id="scraping-specific-html-nodes">③ Scraping Specific HTML Nodes</h2>
<p>However, if we are concerned only with specific content on the webpage then we need to make our HTML node selection process a little more focused.  To do this we, we can use our browser’s developer tools to examine the webpage we are scraping and get more details on specific nodes of interest.  If you are using Chrome or Firefox you can open the developer tools by clicking F12 (Cmd + Opt + I for Mac) or for Safari you would use Command-Option-I. An additional option which is recommended by Hadley Wickham is to use <a href="http://selectorgadget.com/">selectorgadget.com</a>, a Chrome extension, to help identify the web page elements you need<sup><a href="#fn1" id="ref1">1</a></sup>.</p>

<p>Once the developers tools are opened your primary concern is with the element selector. This is located in the top lefthand corner of the developers tools window.</p>

<p><img src="http://bradleyboehmke.github.io/figure/source/scraping-html-text/2015-12-30-scraping-html-text/element_selector.jpg" alt="Element Selector" /></p>

<p>Once you’ve selected the element selector you can now scroll over the elements of the webpage which will cause each element you scroll over to be highlighted.  Once you’ve identified the element you want to focus on, select it. This will cause the element to be identified in the developer tools window. For example, if I am only interested in the main body of the Web Scraping content on the Wikipedia page then I would select the element that highlights the entire center component of the webpage.  This highlights the corresponding element <code class="highlighter-rouge">&lt;div id="bodyContent" class="mw-body-content"&gt;</code> in the developer tools window as the following illustrates.</p>

<p><img src="http://bradleyboehmke.github.io/figure/source/scraping-html-text/2015-12-30-scraping-html-text/body_content_selected.png" alt="Body Content Selected" /></p>

<p>I can now use this information to select and scrape all the text from this specific <code class="highlighter-rouge">&lt;div&gt;</code> node by calling the ID name (“#mw-content-text”) in <code class="highlighter-rouge">html_nodes()</code><sup><a href="#fn2" id="ref2">2</a></sup>.  As you can see below, the text that is scraped begins with the first line in the main body of the Web Scraping content and ends with the text in the <a href="https://en.wikipedia.org/wiki/Web_scraping#See_also_2">See Also</a> section which is the last bit of text directly pertaining to Web Scraping on the webpage. Explicitly, we have pulled the specific text associated with the web content we desire.</p>

<figure class="highlight"><pre><code class="language-r" data-lang="r"><span class="n">body_text</span> <span class="o">&lt;-</span> <span class="n">scraping_wiki</span> <span class="o">%&gt;%</span>
        <span class="n">html_nodes</span><span class="p">(</span><span class="s2">"#mw-content-text"</span><span class="p">)</span> <span class="o">%&gt;%</span> 
        <span class="n">html_text</span><span class="p">()</span>

<span class="c1"># read the first 207 characters
</span><span class="n">substr</span><span class="p">(</span><span class="n">body_text</span><span class="p">,</span> <span class="n">start</span> <span class="o">=</span> <span class="m">1</span><span class="p">,</span> <span class="n">stop</span> <span class="o">=</span> <span class="m">207</span><span class="p">)</span>
<span class="c1">## [1] "Web scraping (web harvesting or web data extraction) is a computer software technique of extracting information from websites. Usually, such software programs simulate human exploration of the World Wide Web"
</span>
<span class="c1"># read the last 73 characters
</span><span class="n">substr</span><span class="p">(</span><span class="n">body_text</span><span class="p">,</span> <span class="n">start</span> <span class="o">=</span> <span class="n">nchar</span><span class="p">(</span><span class="n">body_text</span><span class="p">)</span><span class="m">-73</span><span class="p">,</span> <span class="n">stop</span> <span class="o">=</span> <span class="n">nchar</span><span class="p">(</span><span class="n">body_text</span><span class="p">))</span>
<span class="err">##</span> <span class="p">[</span><span class="m">1</span><span class="p">]</span> <span class="s2">"See also[edit]\n\nData scraping\nData wrangling\nKnowledge extraction\n\n\n\n\n\n\n\n\n"</span></code></pre></figure>

<p>Using the developer tools approach allows us to be as specific as we desire.  We can identify the class name for a specific HTML element and scrape the text for only that node rather than all the other elements with similar tags. This allows us to scrape the main body of content as we just illustrated or we can also identify specific headings, paragraphs, lists, and list components if we desire to scrape only these specific pieces of text:</p>

<figure class="highlight"><pre><code class="language-r" data-lang="r"><span class="c1"># Scraping a specific heading
</span><span class="n">scraping_wiki</span> <span class="o">%&gt;%</span>
        <span class="n">html_nodes</span><span class="p">(</span><span class="s2">"#Techniques"</span><span class="p">)</span> <span class="o">%&gt;%</span> 
        <span class="n">html_text</span><span class="p">()</span>
<span class="c1">## [1] "Techniques"
</span>
<span class="c1"># Scraping a specific paragraph
</span><span class="n">scraping_wiki</span> <span class="o">%&gt;%</span>
        <span class="n">html_nodes</span><span class="p">(</span><span class="s2">"#mw-content-text &gt; p:nth-child(20)"</span><span class="p">)</span> <span class="o">%&gt;%</span> 
        <span class="n">html_text</span><span class="p">()</span>
<span class="c1">## [1] "In Australia, the Spam Act 2003 outlaws some forms of web harvesting, although this only applies to email addresses.[20][21]"
</span>
<span class="c1"># Scraping a specific list
</span><span class="n">scraping_wiki</span> <span class="o">%&gt;%</span>
        <span class="n">html_nodes</span><span class="p">(</span><span class="s2">"#mw-content-text &gt; div:nth-child(22)"</span><span class="p">)</span> <span class="o">%&gt;%</span> 
        <span class="n">html_text</span><span class="p">()</span>
<span class="c1">## [1] "\n\nApache Camel\nArchive.is\nAutomation Anywhere\nConvertigo\ncURL\nData Toolbar\nDiffbot\nFirebug\nGreasemonkey\nHeritrix\nHtmlUnit\nHTTrack\niMacros\nImport.io\nJaxer\nNode.js\nnokogiri\nPhantomJS\nScraperWiki\nScrapy\nSelenium\nSimpleTest\nwatir\nWget\nWireshark\nWSO2 Mashup Server\nYahoo! Query Language (YQL)\n\n"
</span>
<span class="c1"># Scraping a specific reference list item
</span><span class="n">scraping_wiki</span> <span class="o">%&gt;%</span>
        <span class="n">html_nodes</span><span class="p">(</span><span class="s2">"#cite_note-22"</span><span class="p">)</span> <span class="o">%&gt;%</span> 
        <span class="n">html_text</span><span class="p">()</span>
<span class="err">##</span> <span class="p">[</span><span class="m">1</span><span class="p">]</span> <span class="s2">"^ \"Web Scraping: Everything You Wanted to Know (but were afraid to ask)\". Distil Networks. 2015-07-22. Retrieved 2015-11-04. "</span></code></pre></figure>

<p><br /></p>

<p><a name="cleaning"></a></p>

<h2 id="cleaning-up">④ Cleaning up</h2>
<p>With any webscraping activity, especially involving text, there is likely to be some clean-up involved. For example, in the previous example we saw that we can specifically pull the list of <a href="https://en.wikipedia.org/wiki/Web_scraping#Notable_tools"><strong>Notable Tools</strong></a>; however, you can see that in between each list item rather than a space there contains one or more <code class="highlighter-rouge">\n</code> which is used in HTML to specify a new line. We can clean this up quickly with a little <a href="http://bradleyboehmke.github.io/tutorials/string_manipulation">character string manipulation</a>.</p>

<figure class="highlight"><pre><code class="language-r" data-lang="r"><span class="n">library</span><span class="p">(</span><span class="n">magrittr</span><span class="p">)</span>

<span class="n">scraping_wiki</span> <span class="o">%&gt;%</span>
        <span class="n">html_nodes</span><span class="p">(</span><span class="s2">"#mw-content-text &gt; div:nth-child(22)"</span><span class="p">)</span> <span class="o">%&gt;%</span> 
        <span class="n">html_text</span><span class="p">()</span>
<span class="c1">## [1] "\n\nApache Camel\nArchive.is\nAutomation Anywhere\nConvertigo\ncURL\nData Toolbar\nDiffbot\nFirebug\nGreasemonkey\nHeritrix\nHtmlUnit\nHTTrack\niMacros\nImport.io\nJaxer\nNode.js\nnokogiri\nPhantomJS\nScraperWiki\nScrapy\nSelenium\nSimpleTest\nwatir\nWget\nWireshark\nWSO2 Mashup Server\nYahoo! Query Language (YQL)\n\n"
</span>
<span class="n">scraping_wiki</span> <span class="o">%&gt;%</span>
        <span class="n">html_nodes</span><span class="p">(</span><span class="s2">"#mw-content-text &gt; div:nth-child(22)"</span><span class="p">)</span> <span class="o">%&gt;%</span> 
        <span class="n">html_text</span><span class="p">()</span> <span class="o">%&gt;%</span> 
        <span class="n">strsplit</span><span class="p">(</span><span class="n">split</span> <span class="o">=</span> <span class="s2">"\n"</span><span class="p">)</span> <span class="o">%&gt;%</span>
        <span class="n">unlist</span><span class="p">()</span> <span class="o">%&gt;%</span>
        <span class="err">.</span><span class="p">[</span><span class="err">.</span> <span class="o">!=</span> <span class="s2">""</span><span class="p">]</span>
<span class="c1">##  [1] "Apache Camel"                "Archive.is"                 
##  [3] "Automation Anywhere"         "Convertigo"                 
##  [5] "cURL"                        "Data Toolbar"               
##  [7] "Diffbot"                     "Firebug"                    
##  [9] "Greasemonkey"                "Heritrix"                   
## [11] "HtmlUnit"                    "HTTrack"                    
## [13] "iMacros"                     "Import.io"                  
## [15] "Jaxer"                       "Node.js"                    
## [17] "nokogiri"                    "PhantomJS"                  
## [19] "ScraperWiki"                 "Scrapy"                     
## [21] "Selenium"                    "SimpleTest"                 
## [23] "watir"                       "Wget"                       
## [25] "Wireshark"                   "WSO2 Mashup Server"         
</span><span class="err">##</span> <span class="p">[</span><span class="m">27</span><span class="p">]</span> <span class="s2">"Yahoo! Query Language (YQL)"</span></code></pre></figure>

<p>Similarly, as we saw in our example above with scraping the main body content (<code class="highlighter-rouge">body_text</code>), there are extra characters (i.e. <code class="highlighter-rouge">\n</code>, <code class="highlighter-rouge">\</code>, <code class="highlighter-rouge">^</code>) in the text that we may not want.  Using a <a href="http://bradleyboehmke.github.io/tutorials/regex">little regex</a> we can clean this up so that our character string consists of only text that we see on the screen and no additional HTML code embedded throughout the text.</p>

<figure class="highlight"><pre><code class="language-r" data-lang="r"><span class="n">library</span><span class="p">(</span><span class="n">stringr</span><span class="p">)</span>

<span class="c1"># read the last 700 characters
</span><span class="n">substr</span><span class="p">(</span><span class="n">body_text</span><span class="p">,</span> <span class="n">start</span> <span class="o">=</span> <span class="n">nchar</span><span class="p">(</span><span class="n">body_text</span><span class="p">)</span><span class="m">-700</span><span class="p">,</span> <span class="n">stop</span> <span class="o">=</span> <span class="n">nchar</span><span class="p">(</span><span class="n">body_text</span><span class="p">))</span>
<span class="c1">## [1] " 2010). \"Intellectual Property: Website Terms of Use\". Issue 26: June 2010. LK Shields Solicitors Update. p. 03. Retrieved 2012-04-19. \n^ National Office for the Information Economy (February 2004). \"Spam Act 2003: An overview for business\" (PDF). Australian Communications Authority. p. 6. Retrieved 2009-03-09. \n^ National Office for the Information Economy (February 2004). \"Spam Act 2003: A practical guide for business\" (PDF). Australian Communications Authority. p. 20. Retrieved 2009-03-09. \n^ \"Web Scraping: Everything You Wanted to Know (but were afraid to ask)\". Distil Networks. 2015-07-22. Retrieved 2015-11-04. \n\n\nSee also[edit]\n\nData scraping\nData wrangling\nKnowledge extraction\n\n\n\n\n\n\n\n\n"
</span>
<span class="c1"># clean up text
</span><span class="n">body_text</span> <span class="o">%&gt;%</span>
        <span class="n">str_replace_all</span><span class="p">(</span><span class="n">pattern</span> <span class="o">=</span> <span class="s2">"\n"</span><span class="p">,</span> <span class="n">replacement</span> <span class="o">=</span> <span class="s2">" "</span><span class="p">)</span> <span class="o">%&gt;%</span>
        <span class="n">str_replace_all</span><span class="p">(</span><span class="n">pattern</span> <span class="o">=</span> <span class="s2">"[\\^]"</span><span class="p">,</span> <span class="n">replacement</span> <span class="o">=</span> <span class="s2">" "</span><span class="p">)</span> <span class="o">%&gt;%</span>
        <span class="n">str_replace_all</span><span class="p">(</span><span class="n">pattern</span> <span class="o">=</span> <span class="s2">"\""</span><span class="p">,</span> <span class="n">replacement</span> <span class="o">=</span> <span class="s2">" "</span><span class="p">)</span> <span class="o">%&gt;%</span>
        <span class="n">str_replace_all</span><span class="p">(</span><span class="n">pattern</span> <span class="o">=</span> <span class="s2">"\\s+"</span><span class="p">,</span> <span class="n">replacement</span> <span class="o">=</span> <span class="s2">" "</span><span class="p">)</span> <span class="o">%&gt;%</span>
        <span class="n">str_trim</span><span class="p">(</span><span class="n">side</span> <span class="o">=</span> <span class="s2">"both"</span><span class="p">)</span> <span class="o">%&gt;%</span>
        <span class="n">substr</span><span class="p">(</span><span class="n">start</span> <span class="o">=</span> <span class="n">nchar</span><span class="p">(</span><span class="n">body_text</span><span class="p">)</span><span class="m">-700</span><span class="p">,</span> <span class="n">stop</span> <span class="o">=</span> <span class="n">nchar</span><span class="p">(</span><span class="n">body_text</span><span class="p">))</span>
<span class="err">##</span> <span class="p">[</span><span class="m">1</span><span class="p">]</span> <span class="s2">"012-04-19. National Office for the Information Economy (February 2004). Spam Act 2003: An overview for business (PDF). Australian Communications Authority. p. 6. Retrieved 2009-03-09. National Office for the Information Economy (February 2004). Spam Act 2003: A practical guide for business (PDF). Australian Communications Authority. p. 20. Retrieved 2009-03-09. Web Scraping: Everything You Wanted to Know (but were afraid to ask) . Distil Networks. 2015-07-22. Retrieved 2015-11-04. See also[edit] Data scraping Data wrangling Knowledge extraction"</span></code></pre></figure>

<p><br /></p>

<h2 id="wrapping-up">Wrapping up</h2>
<p>So there we have it, text scraping in a nutshell.  Although not all encompassing, this post covered the basics of scraping text from HTML documents. Whether you want to scrape text from all common text-containing nodes such as <code class="highlighter-rouge">&lt;div&gt;</code>, <code class="highlighter-rouge">&lt;p&gt;</code>, <code class="highlighter-rouge">&lt;ul&gt;</code> and the like or you want to scrape from a specific node using the specific ID, this post provides you the basic fundamentals of using <code class="highlighter-rouge">rvest</code> to scrape the text you need. Now go forth and scrape!</p>

<p><small><a href="#">Go to top</a></small></p>

<p class="footnote" style="line-height:0.75">
<sup id="fn1">1. You can learn more about selectors at <a href="http://flukeout.github.io/">flukeout.github.io</a><a href="#ref1" title="Jump back to footnote 1 in the text.">"&#8617;"</a><sup>

<p class="footnote" style="line-height:0.75">
<sup id="fn2">2. You can simply assess the name of the ID in the highlighted element or you can  right click the highlighted element in the developer tools window and select <em>Copy selector</em>.  You can then paste directly into `html_nodes()` as it will paste the exact ID name that you need for that element.<a href="#ref2" title="Jump back to footnote 2 in the text.">"&#8617;"</a><sup>


</sup></sup></p></sup></sup></p>

  </article>

</div>





      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col  footer-col-1">

      </div>

      <div class="footer-col  footer-col-2">
        <ul class="social-media-list">
        <center>
          <li>
            <a href="https://github.com/bradleyboehmke">
              <span class="icon  icon--github">
                <svg viewBox="0 0 16 16">
                  <path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
                </svg>
              </span>
            </a>
            &nbsp;
            <a href="http://www.twitter.com/bradleyboehmke">
              <span class="icon  icon--twitter">
                <svg viewBox="0 0 16 16">
                  <path fill="#828282" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809
                  c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/>
                </svg>
              </span>
            </a>
            &nbsp;
            <a href="https://www.linkedin.com/in/brad-boehmke-ph-d-9b0a257">
              <span class="icon  icon--linkedin">
                <svg viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg"  width="25" height="25" fill-rule="evenodd" clip-rule="evenodd" stroke-linejoin="round" stroke-miterlimit="1.414"><path fill="#828282" d="M13.632 13.635h-2.37V9.922c0-.886-.018-2.025-1.234-2.025-1.235 0-1.424.964-1.424 1.96v3.778h-2.37V6H8.51V7.04h.03c.318-.6 1.092-1.233 2.247-1.233 2.4 0 2.845 1.58 2.845 3.637v4.188zM3.558 4.955c-.762 0-1.376-.617-1.376-1.377 0-.758.614-1.375 1.376-1.375.76 0 1.376.617 1.376 1.375 0 .76-.617 1.377-1.376 1.377zm1.188 8.68H2.37V6h2.376v7.635zM14.816 0H1.18C.528 0 0 .516 0 1.153v13.694C0 15.484.528 16 1.18 16h13.635c.652 0 1.185-.516 1.185-1.153V1.153C16 .516 15.467 0 14.815 0z" fill-rule="nonzero"/>
                </svg>
              </span>
            </a>
          </li>

        </ul>
        </center>
      </div>

      <div class="footer-col  footer-col-3">
        <p class="text"></p>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>
