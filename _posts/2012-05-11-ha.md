---
id: 199
title: 'Interview with Hadley Wickham - Developer of ggplot2'
date: 2012-05-11T16:11:20+00:00
author: admin
layout: post
guid: http://simplystatistics.wordpress.com/2012/05/11/ha
permalink: /2012/05/11/ha/
tumblr_simplystatistics_id:
  - 22844703875
tumblr_simplystatistics_permalink:
  - http://simplystatistics.tumblr.com/post/22844703875/ha
dsq_thread_id:
  - 934631485
categories:
  - Uncategorized
tags:
  - data visualization
  - ggplot2
  - hadley wickham
  - interview
---
<div class="im">
  <strong>Hadley Wickham</strong>
</div>

<div class="im">
  <strong><br /></strong>
</div>

<div class="im">
  <strong><img height="365" src="http://biostat.jhsph.edu/~jleek/hw.jpg" width="244" /></strong>
</div>

<div class="im">
  <strong><br /></strong>
</div>

<div class="im">
  <strong><br /></strong><a href="http://had.co.nz/" target="_blank">Hadley Wickham </a>is the Dobelman Family Junior Chair of Statistics at Rice University. Prior to moving to Rice, he completed his Ph.D. in Statistics from Iowa State University. He is the developer of the wildly popular <a href="http://had.co.nz/ggplot2/" target="_blank">ggplot2</a> software for data visualization and a contributor to the <a href="http://www.ggobi.org/" target="_blank">Ggobi </a>project. He has developed a number of really useful R packages touching everything from data processing, to data modeling, to visualization. 
</div>

<div class="im">
  <strong><br /></strong>
</div>

<div class="im">
  <strong>Which term applies to you: data scientist, statistician, computer</strong><br /><strong>scientist, or something else?</strong></p>
</div>

<span>I&#8217;m an assistant professor of statistics, so I at least partly</span>  
<span>associate with statistics :).  But the idea of data science really</span>  
<span>resonates with me: I like the combination of tools from statistics and</span>  
<span>computer science, data analysis and hacking, with the core goal of</span>  
<span>developing a better understanding of data. Sometimes it seems like not</span>  
<span>much statistics research is actually about gaining insight into data.</span>

<div class="im">
  <strong>You have created/maintain several widely used R packages. Can you</strong><br /><strong>describe the unique challenges to writing and maintaining packages</strong><br /><strong>above and beyond developing the methods themselves?</strong></p>
</div>

I think there are two main challenges: turning ideas into code, and  
documentation and community building.

Compared to other languages, the software development infrastructure  
in R is weak, which sometimes makes it harder than necessary to turn  
my ideas into code. Additionally, I get less and less time to do  
software development, so I can&#8217;t afford to waste time recreating old  
bugs, or releasing packages that don&#8217;t work. Recently, I&#8217;ve been  
investing time in helping build better dev infrastructure; better  
tools for documentation <a href="http://github.com/klutometis/roxygen" target="_blank">[roxygen2]</a>, unit testing <a href="https://github.com/hadley/test_that" target="_blank">[testthat]</a>, package development <a href="https://github.com/hadley/devtools" target="_blank">[devtools]</a>, and creating package website <a href="https://github.com/hadley/staticdocs" target="_blank">[staticdocs]</a>. Generally, I&#8217;ve  
found unit tests to be a worthwhile investment: they ensure you never  
accidentally recreate an old bug, and give you more confidence when  
radically changing the implementation of a function.

Documenting code is hard work, and it&#8217;s certainly something I haven&#8217;t  
mastered. But documentation is absolutely crucial if you want people  
to use your work. I find the main challenge is putting yourself in the  
mind of the new user: what do they need to know to use the package  
effectively. This is really hard to do as a package author because  
you&#8217;ve internalised both the motivating problem and many of the common  
solutions.

Connected to documentation is building up a community around your  
work. This is important to get feedback on your package, and can be  
helpful for reducing the support burden. One of the things I&#8217;m most  
proud of about ggplot2 is something that I&#8217;m barely responsible for:  
the ggplot2 mailing list. There are now ggplot2 experts who answer far  
more questions on the list than I do. I&#8217;ve also found github to be  
great: there&#8217;s an increasing community of users proficient in both R  
and git who produce pull requests that fix bugs and add new features.

The flip side of building a community is that as your work becomes  
more popular you need to be more careful when releasing new versions.  
The last major release of ggplot2 (0.9.0) broke over 40 (!!) CRAN  
packages, and forced me to rethink my release process. Now I advertise  
releases a month in advance, and run \`R CMD check\` on all downstream  
dependencies (\`devtools::revdep_check\` in the development version), so  
I can pick up potential problems and give other maintainers time to  
fix any issues.

<div class="im">
  <strong>Do you feel that the academic culture has caught up with and supports</strong><br /><strong>non-traditional academic contributions (e.g. R packages instead of</strong><br /><strong>papers)?</strong></p>
</div>

<span>It&#8217;s hard to tell. I think it&#8217;s getting better, but it&#8217;s still hard to</span>  
<span>get recognition that software development is an intellectual activity</span>  
<span>in the same way that developing a new mathematical theorem is. I try</span>  
<span>to hedge my bets by publishing papers to accompany my major packages:</span>  
<span>I&#8217;ve also found the peer-review process very useful for improving the</span>  
<span>quality of my software. Reviewers from both the R journal and the</span>  
<span>Journal of Statistical Software have provided excellent suggestions</span>  
<span>for enhancements to my code.</span>

<div class="im">
  <strong>You have given presentations at several start-up and tech companies.</strong><br /><strong>Do the corporate users of your software have different interests than</strong><br /><strong>the academic users?</strong></p>
</div>

<span>By and large, no. Everyone, regardless of domain, is struggling to</span>  
<span>understand ever larger datasets. Across both industry and academia,</span>  
<span>practitioners are worried about reproducible research and thinking</span>  
<span>about how to apply the principles of software engineering to data</span>  
<span>analysis.</span>

<div class="im">
  <strong>You gave one of my favorite presentations called Tidy Data/Tidy Tools</strong><br /><strong>at the NYC Open Statistical Computing Meetup. What are the key</strong><br /><strong>elements of tidy data that all applied statisticians should know?</strong></p>
</div>

Thanks! Basically, make sure you store your data in a consistent  
format, and pick (or develop) tools that work with that data format.  
The more time you spend munging data in the middle of an analysis, the  
less time you have to discover interesting things in your data. I&#8217;ve  
tried to develop a consistent philosophy of data that means when you  
use my packages (particularly <a href="http://plyr.had.co.nz/" target="_blank">plyr</a> and <a href="http://had.co.nz/ggplot2/" target="_blank">ggplot2</a>), you can focus on the  
data analysis, not on the details of the data format. The principles  
of tidy data that I adhere to are that every column should be a  
variable, every row an observation, and different types of data should  
live in different data frames. (If you&#8217;re familiar with database  
normalisation this should sound pretty familiar!). I expound these  
principles in depth in my in-progress <a href="http://vita.had.co.nz/papers/tidy-data.html" target="_blank">[paper on the<br />topic]</a>. 

<div class="im">
  <strong>How do you decide what project to work on next? Is your work inspired</strong><br /><strong>by a particular application or more general problems you are trying to</strong><br /><strong>tackle?</strong></p>
</div>

Very broadly, I&#8217;m interested in the whole process of data analysis:  
the process that takes raw data and converts it into understanding,  
knowledge and insight. I&#8217;ve identified three families of tools  
(manipulation, modelling and visualisation) that are used in every  
data analysis, and I&#8217;m interested both in developing better individual  
tools, but also smoothing the transition between them. In every good  
data analysis, you must iterate multiple times between manipulation,  
modelling and visualisation, and anything you can do to make that  
iteration faster yields qualitative improvements to the final analysis  
(that was one of the driving reasons I&#8217;ve been working on tidy data).

Another factor that motivates a lot of my work is teaching. I hate  
having to teach a topic that&#8217;s just a collection of special cases,  
with no underlying theme or theory. That drive lead to <a href="http://cran.r-project.org/web/packages/stringr/index.html" target="_blank">[stringr]</a> (for  
string manipulation) and <a href="http://cran.r-project.org/web/packages/lubridate/index.html" target="_blank">[lubridate]</a> (with Garrett Grolemund for working  
with dates). I recently released the <a href="https://github.com/hadley/httr" target="_blank">[httr]</a> package which aims to do a similar thing for http requests - I think this is particularly important as more and more data starts living on the web and must be accessed through an API.

<div class="im">
  <strong>What do you see as the biggest open challenges in data visualization</strong><br /><strong>right now? Do you see interactive graphics becoming more commonplace?</strong></p>
</div>

I think one of the biggest challenges for data visualisation is just  
communicating what we know about good graphics. The first article  
decrying 3d bar charts was <a href="http://www.jstor.org/stable/2682265" target="_blank">published in 1951</a>! Many plots still use  
rainbow scales or red-green colour contrasts, even though we&#8217;ve known  
for decades that those are bad. How can we ensure that people  
producing graphics know enough to do a good job, without making them  
read hundreds of papers? It&#8217;s a really hard problem.

Another big challenge is balancing the tension between exploration and  
presentation. For explotary graphics, you want to spend five seconds  
(or less) to create a plot that helps you understand the data, while you might spend  
five hours on a plot that&#8217;s persuasive to an audience who  
isn&#8217;t as intimately familiar with the data as you. To date, we have  
great interactive graphics solutions at either end of the spectrum  
(e.g. ggobi/iplots/manet vs d3) but not much that transitions from one  
end of the spectrum to the other. This summer I&#8217;ll be spending some  
time thinking about what ggplot2 + <a href="http://d3js.org/" target="_blank">[d3]</a>, might  
equal, and how we can design something like an interactive grammar of  
graphics that lets you explore data in R, while making it easy to  
publish interaction presentation graphics on the web.