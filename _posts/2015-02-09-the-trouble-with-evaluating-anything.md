---
id: 3889
title: The trouble with evaluating anything
date: 2015-02-09T19:24:22+00:00
author: Jeff Leek
layout: post
guid: http://simplystatistics.org/?p=3889
permalink: /2015/02/09/the-trouble-with-evaluating-anything/
pe_theme_meta:
  - 'O:8:"stdClass":2:{s:7:"gallery";O:8:"stdClass":3:{s:2:"id";s:2:"-1";s:5:"width";s:0:"";s:6:"height";s:0:"";}s:5:"video";O:8:"stdClass":1:{s:2:"id";s:2:"-1";}}'
al2fb_facebook_link_id:
  - 136171103105421_766703000052225
al2fb_facebook_link_time:
  - 2015-02-10T00:24:28+00:00
al2fb_facebook_link_picture:
  - post=http://simplystatistics.org/?al2fb_image=1
dsq_thread_id:
  - 3501108926
categories:
  - Uncategorized
---
It is very hard to evaluate people's productivity or work in any meaningful way. This problem is the source of:

  1. [Consternation about peer review](http://simplystatistics.org/2013/09/26/how-could-code-review-discourage-code-disclosure-reviewers-with-motivation/)
  2. [The reason why post publication peer review doesn't work](http://simplystatistics.org/2014/02/21/heres-why-the-scientific-publishing-system-can-never-be-fixed/)
  3. [Consternation about faculty evaluation](http://simplystatistics.org/2012/05/24/how-do-we-evaluate-statisticians-working-in-genomics/)
  4. Major problems at companies like [Yahoo](http://www.bloomberg.com/bw/articles/2013-11-12/yahoos-latest-hr-disaster-ranking-workers-on-a-curve) and [Microsoft](http://www.bloomberg.com/bw/articles/2013-11-13/microsoft-kills-its-hated-stack-rankings-dot-does-anyone-do-employee-reviews-right).

Roger and I were just talking about this problem in the context of evaluating the impact of software as a faculty member and Roger suggested the problem is that:

> Evaluating people requires real work and so people are always looking for shortcuts

To evaluate a person's work or their productivity requires three things:

  1. To be an expert in what they do
  2. To have absolutely no reason to care whether they succeed or not
  3. To have time available to evaluate them

These three fundamental things are at the heart of why it is so hard to get good evaluations of people and why peer review and other systems are under such fire. The main source of the problem is the conflict between 1 and 2. The group of people in any organization or on any scale that is truly world class at any given topic from software engineering to history is small. It has to be by definition. This group of people inevitably has some reason to care about the success of the other people in that same group. Either they work with the other world class people and want them to succeed or they  either intentionally or unintentionally are competing with them.

The conflict between being and expert and having no say wouldn't be such a problem if it wasn't for issue number 3: the time to evaluate people. To truly get good evaluations what you need is for someone who _isn't an expert in a field and so has no stake_ to take the time to become an expert and then evaluate the person/software. But this requires a huge amount of effort on the part of a reviewer who has to become expert in a new field. Given that reviewing is often considered the least important task in people's workflow, evidenced by the value we put on people acting as peer reviewers for journals, or the value people get for doing a good job in people's evaluation for promotion in companies, it is no wonder people don't take the time to become experts.

I actually think that tenure review committees at forward thinking places may be the best at this ([Rafa said the same thing about NIH study section](http://simplystatistics.org/2012/12/20/the-nih-peer-review-system-is-still-the-best-at-identifying-innovative-biomedical-investigators/)). They at least attempt to get outside reviews from people who are unbiased about the work that a faculty member is doing before they are promoted. This system, of course, has large and well-document problems, but I think it is better than having a person's direct supervisor - who clearly has a stake - being the only person evaluating them.It is also better than only using the quantifiable metrics like number of papers and impact factor of the corresponding journals. I also think that most senior faculty who evaluate people take the job very seriously despite the only incentive being good citizenship.

Since real evaluation requires hard work and expertise, most of the time people are looking for a short cut. These short cuts typically take the form of quantifiable metrics. In the academic world these shortcuts are things like:

  1. Number of papers
  2. Citations to academic papers
  3. The impact factor of a journal
  4. Downloads to a person's software

I think all of these things are associated with quality but none define quality. You could try to model the relationship, but it is very hard to come up with a universal definition for the outcome you are trying to model. In academics, some people have suggested that [open review or post-publication review](http://www.michaeleisen.org/blog/?p=694) solves the problem. But this is only true for a very small subset of cases that violate rule number 2. The only papers that get serious post-publication review are where people have an incentive for the paper to go one way or the other. This means that papers in Science will be post-pub reviewed much much more often than equally important papers in discipline specific journals - just because people care more about Science. This will leave the vast majority of papers unreviewed - as evidenced by the relatively modest number of papers reviewed by [PubPeer](https://pubpeer.com/) or [Pubmed Commons.](http://www.ncbi.nlm.nih.gov/pubmedcommons/)

I'm beginning to think that the only way to do evaluation well is to hire people whose _only job is to evaluate something well_. In other words, peer reviewers who are paid to review papers full time and are only measured by how often those papers are retracted or proved false. Or tenure reviewers who are paid exclusively to evaluate tenure cases and are measured by how well the post-tenure process goes for the people they evaluate and whether there is any measurable bias in their reviews.

The trouble with evaluating anything is that it is hard work and right now we aren't paying anyone to do it.

&nbsp;