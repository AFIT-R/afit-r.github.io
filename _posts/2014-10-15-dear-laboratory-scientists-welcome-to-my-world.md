---
id: 3377
title: 'Dear Laboratory Scientists: Welcome to My World'
date: 2014-10-15T19:42:03+00:00
author: Roger Peng
layout: post
guid: http://simplystatistics.org/?p=3377
permalink: /2014/10/15/dear-laboratory-scientists-welcome-to-my-world/
al2fb_facebook_link_id:
  - 136171103105421_712280978827761
al2fb_facebook_link_time:
  - 2014-10-15T23:42:08+00:00
al2fb_facebook_link_picture:
  - post=http://simplystatistics.org/?al2fb_image=1
dsq_thread_id:
  - 3121151745
categories:
  - Uncategorized
---
Consider the following question: Is there a reproducibility/replication crisis in epidemiology?

I think there are only two possible ways to answer that question:

  1. No, there is no replication crisis in epidemiology because no one ever believes the result of an epidemiological study unless it has been replicated a minimum of 1,000 times in every possible population.
  2. Yes, there is a replication crisis in epidemiology, and it started in 1854 when [John Snow](http://www.ph.ucla.edu/epi/snow/snowbook2.html) inferred, from observational data, that cholera was spread via contaminated water obtained from public pumps.

If you chose (2), then I don't think you are allowed to call it a "crisis" because I think by definition, a crisis cannot last 160 years. In that case, it's more of a chronic disease.

I had an interesting conversation last week with a prominent environmental epidemiologist over the replication crisis that has been reported about extensively in the scientific and popular press. In his view, he felt this was less of an issue in epidemiology because epidemiologists never really had the luxury of people (or at least fellow scientists) believing their results because of their general inability to conduct controlled experiments.

Given the observational nature of most environmental epidemiological studies, it's generally accepted in the community that no single study can be considered causal, and that many replications of a finding are need to establish a causal connection. Even the popular press knows now to include the phrase "correlation does not equal causation" when reporting on an observational study. The work of [Sir Austin Bradford Hill](http://en.wikipedia.org/wiki/Bradford_Hill_criteria) essentially codifies the standard of evidence needed to draw causal conclusions from observational studies.

So if "correlation does not equal causation", it begs the question, what _does_ equal causation? Many would argue that a controlled experiment, whether it's a randomized trial or a laboratory experiment, equals causation. But people who work in this area have long known that while controlled experiments do assign the treatment or exposure, there are still many other elements of the experiment that are _not _controlled.

For example, if subjects drop out of a randomized trial, you now essentially have an observational study (or at least a ["broken" randomized trial](http://amstat.tandfonline.com/doi/abs/10.1198/016214503000071#.VD8EqL5DuoY)). If you are conducting a laboratory experiment and all of the treatment samples are measured with one technology and all of the control samples are measured with a different technology (perhaps because of a lack of blinding), then you still have confounding.

The correct statement is not "correlation does not equal causation" but rather "no single study equals causation", regardless of whether it was an observational study or a controlled experiment. Of course, a very tightly controlled and rigorously conducted controlled experiment will be more valuable than a similarly conducted observational study. But in general, all studies should simply be considered as further evidence for or against an hypothesis. We should not be lulled into thinking that any single study about an important question can truly be definitive.